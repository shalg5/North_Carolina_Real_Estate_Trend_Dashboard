{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95452365-d095-4bec-8d9d-31ce94ee799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ ZHVI: reading data/raw/ZHVI.csv\n",
      "Raw: (21489, 314)\n",
      "Date columns: 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\590637863.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: fill_small_gaps(g, metric_name, ffill_limit, interpolate_limit))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ZHVI: (84294, 10) | Duplicates (RegionID+Date): 0\n",
      "âœ… Saved: data\\interim\\nc_city_ZHVI_2015present.csv\n",
      "\n",
      "ðŸ“¥ ZORI: reading data/raw/ZORI.csv\n",
      "Raw: (3768, 134)\n",
      "Date columns: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\590637863.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: fill_small_gaps(g, metric_name, ffill_limit, interpolate_limit))\n",
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\590637863.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: fill_small_gaps(g, metric_name, ffill_limit, interpolate_limit))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ZORI: (15120, 10) | Duplicates (RegionID+Date): 0\n",
      "âœ… Saved: data\\interim\\nc_city_ZORI_2015present.csv\n",
      "\n",
      "ðŸ“¥ Inventory: reading data/raw/Inventory.csv\n",
      "Raw: (928, 93)\n",
      "Date columns: 88\n",
      "Final Inventory: (3256, 7) | Duplicates (RegionID+Date): 0\n",
      "âœ… Saved: data\\interim\\nc_city_Inventory_2015present.csv\n",
      "\n",
      "ðŸ“¥ DaysToPending: reading data/raw/DaysToPending.csv\n",
      "Raw: (756, 93)\n",
      "Date columns: 88\n",
      "Final DaysToPending: (2640, 7) | Duplicates (RegionID+Date): 0\n",
      "âœ… Saved: data\\interim\\nc_city_DaysToPending_2015present.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\590637863.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: fill_small_gaps(g, metric_name, ffill_limit, interpolate_limit))\n"
     ]
    }
   ],
   "source": [
    "# clean_zillow_city_separate.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- CONFIG --------\n",
    "STATE_ABBR = \"NC\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "\n",
    "# Point these to your raw files\n",
    "FILES = {\n",
    "    \"ZHVI\":          \"data/raw/ZHVI.csv\",\n",
    "    \"ZORI\":          \"data/raw/ZORI.csv\",\n",
    "    \"Inventory\":     \"data/raw/Inventory.csv\",\n",
    "    \"DaysToPending\": \"data/raw/DaysToPending.csv\",\n",
    "    # \"PriceCuts\":   \"data/raw/PriceCuts.csv\",  # add when you have it\n",
    "}\n",
    "\n",
    "# Missing-data policy per metric\n",
    "POLICY = {\n",
    "    # prices/rents: no forward-fill, small internal interpolation only\n",
    "    \"ZHVI\":          dict(drop_nonpositive=True,  ffill_limit=0, interpolate_limit=3),\n",
    "    \"ZORI\":          dict(drop_nonpositive=True,  ffill_limit=0, interpolate_limit=3),\n",
    "    # reporting-lag metrics: allow 1-mo ffill, tiny interpolation\n",
    "    \"Inventory\":     dict(drop_nonpositive=False, ffill_limit=1, interpolate_limit=2),\n",
    "    \"DaysToPending\": dict(drop_nonpositive=False, ffill_limit=1, interpolate_limit=2),\n",
    "    # \"PriceCuts\":   dict(drop_nonpositive=False, ffill_limit=0, interpolate_limit=2),\n",
    "}\n",
    "\n",
    "OUT_DIR = Path(\"data/interim\")  # per-metric outputs go here\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ID_COLS = [\"RegionID\",\"SizeRank\",\"RegionName\",\"RegionType\",\"StateName\",\"State\",\"Metro\",\"CountyName\"]\n",
    "\n",
    "# -------- HELPERS --------\n",
    "def detect_date_columns(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Columns whose header can be parsed as a date (any common format).\"\"\"\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if c in ID_COLS: \n",
    "            continue\n",
    "        if not any(ch.isdigit() for ch in c):\n",
    "            continue\n",
    "        dt = pd.to_datetime(c, errors=\"coerce\")\n",
    "        if pd.isna(dt):\n",
    "            dt = pd.to_datetime(c, errors=\"coerce\", dayfirst=True)\n",
    "        if not pd.isna(dt):\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "def state_mask(df: pd.DataFrame, abbr: str) -> pd.Series:\n",
    "    if \"State\" in df.columns:\n",
    "        return df[\"State\"].astype(str).str.upper().eq(abbr)\n",
    "    if \"StateName\" in df.columns:\n",
    "        s = df[\"StateName\"].astype(str).str.upper()\n",
    "        return s.eq(abbr) | s.eq(\"NORTH CAROLINA\")\n",
    "    raise ValueError(\"No State/StateName column found.\")\n",
    "\n",
    "def fill_small_gaps(g: pd.DataFrame, metric: str, ffill_limit: int, interpolate_limit: int) -> pd.DataFrame:\n",
    "    g = g.sort_values(\"Date\").copy()\n",
    "    if ffill_limit > 0:\n",
    "        g[metric] = g[metric].ffill(limit=ffill_limit)  # handle 1-month reporting lag\n",
    "    g[metric] = g[metric].interpolate(\n",
    "        method=\"linear\",\n",
    "        limit=interpolate_limit,        # fill small internal runs only\n",
    "        limit_direction=\"both\",\n",
    "        limit_area=\"inside\"\n",
    "    )\n",
    "    return g\n",
    "\n",
    "def clean_zillow_city_file(\n",
    "    file_path: str,\n",
    "    metric_name: str,\n",
    "    state_abbr: str = STATE_ABBR,\n",
    "    start_date: str = START_DATE,\n",
    "    drop_nonpositive: bool = True,\n",
    "    ffill_limit: int = 0,\n",
    "    interpolate_limit: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    print(f\"\\nðŸ“¥ {metric_name}: reading {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding=\"latin-1\")\n",
    "    print(\"Raw:\", df.shape)\n",
    "\n",
    "    # Tidy headers / strings\n",
    "    df.columns = df.columns.str.strip()\n",
    "    obj = df.select_dtypes(include=\"object\").columns\n",
    "    df[obj] = df[obj].apply(lambda s: s.astype(str).str.strip())\n",
    "\n",
    "    # Detect date headers and melt to long\n",
    "    date_cols = detect_date_columns(df)\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"No date-like columns detected.\")\n",
    "    print(f\"Date columns: {len(date_cols)}\")\n",
    "    id_cols = [c for c in ID_COLS if c in df.columns]\n",
    "\n",
    "    long = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                   var_name=\"DateStr\", value_name=metric_name)\n",
    "    # Parse header dates (any format)\n",
    "    dt = pd.to_datetime(long[\"DateStr\"], errors=\"coerce\")\n",
    "    dt_na = dt.isna()\n",
    "    if dt_na.any():\n",
    "        dt.loc[dt_na] = pd.to_datetime(long.loc[dt_na, \"DateStr\"], errors=\"coerce\", dayfirst=True)\n",
    "    long[\"Date\"] = dt\n",
    "    long = long.drop(columns=[\"DateStr\"]).dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Metric to numeric; treat <=0 as missing for price/rent\n",
    "    long[metric_name] = pd.to_numeric(long[metric_name], errors=\"coerce\")\n",
    "    if drop_nonpositive:\n",
    "        long.loc[long[metric_name] <= 0, metric_name] = pd.NA\n",
    "\n",
    "    # Filters\n",
    "    long = long[state_mask(long, state_abbr)]\n",
    "    long = long[long[\"Date\"] >= pd.Timestamp(start_date)]\n",
    "\n",
    "    # De-dup, then fill small gaps per city\n",
    "    long = (long.sort_values([\"RegionID\",\"Date\"])\n",
    "                .drop_duplicates(subset=[\"RegionID\",\"Date\"], keep=\"last\")\n",
    "                .groupby(\"RegionID\", group_keys=False)\n",
    "                .apply(lambda g: fill_small_gaps(g, metric_name, ffill_limit, interpolate_limit))\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "    # Final tidy columns\n",
    "    keep = [c for c in ID_COLS + [\"Date\", metric_name] if c in long.columns]\n",
    "    long = long[keep].sort_values([\"RegionName\",\"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    # Sanity\n",
    "    dups = long.duplicated(subset=[\"RegionID\",\"Date\"]).sum()\n",
    "    print(f\"Final {metric_name}: {long.shape} | Duplicates (RegionID+Date): {dups}\")\n",
    "    return long\n",
    "\n",
    "# -------- RUN: PER-METRIC OUTPUTS ONLY --------\n",
    "if __name__ == \"__main__\":\n",
    "    for metric, path in FILES.items():\n",
    "        p = POLICY.get(metric, dict(drop_nonpositive=False, ffill_limit=0, interpolate_limit=2))\n",
    "        df_clean = clean_zillow_city_file(\n",
    "            file_path=path,\n",
    "            metric_name=metric,\n",
    "            drop_nonpositive=p[\"drop_nonpositive\"],\n",
    "            ffill_limit=p[\"ffill_limit\"],\n",
    "            interpolate_limit=p[\"interpolate_limit\"],\n",
    "        )\n",
    "        out_path = OUT_DIR / f\"nc_city_{metric}_2015present.csv\"\n",
    "        df_clean.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"âœ… Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960d0a9e-4415-4b4e-ab81-471842db1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\2309668832.py:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/interim/tableau/nc_city_ZHVI_2015present.csv  (84294, 10)\n",
      "Saved: data/interim/tableau/nc_city_ZORI_2015present.csv  (15120, 10)\n",
      "Saved: data/interim/tableau/nc_city_Inventory_2015present.csv  (3256, 7)\n",
      "Saved: data/interim/tableau/nc_city_DaysToPending_2015present.csv  (2640, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\2309668832.py:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\2309668832.py:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
      "C:\\Users\\Shalin\\AppData\\Local\\Temp\\ipykernel_53888\\2309668832.py:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, csv, re\n",
    "from pathlib import Path\n",
    "\n",
    "def tableau_proof(in_path, out_path):\n",
    "    # 1) read robustly, strip nulls\n",
    "    raw = Path(in_path).read_text(encoding=\"utf-8\", errors=\"replace\").replace(\"\\x00\", \"\")\n",
    "    Path(in_path).write_text(raw, encoding=\"utf-8\")\n",
    "\n",
    "    df = pd.read_csv(in_path, engine=\"python\")  # lets pandas sniff delimiter\n",
    "\n",
    "    # 2) tidy headers/strings, drop unnamed\n",
    "    df.columns = [re.sub(r\"\\s+\", \" \", c).strip() for c in df.columns]\n",
    "    df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "\n",
    "    # 3) normalize Date to ISO\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"Date\"])\n",
    "        df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 4) force numeric where possible (without changing text cols)\n",
    "    id_like = {\"RegionID\",\"SizeRank\",\"RegionName\",\"RegionType\",\"StateName\",\"State\",\"Metro\",\"CountyName\",\"Date\"}\n",
    "    for c in df.columns:\n",
    "        if c not in id_like:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "    # 5) write CSV with BOM + CRLF + quote-all (Tableau-safe)\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(\n",
    "        out_path,\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",        # BOM\n",
    "        lineterminator=\"\\r\\n\",       # Windows line endings\n",
    "        quoting=csv.QUOTE_ALL        # commas inside fields are safe\n",
    "    )\n",
    "    print(f\"Saved: {out_path}  {df.shape}\")\n",
    "\n",
    "# Example: point to your cleaned files\n",
    "tableau_proof(\"data/interim/nc_city_ZHVI_2015present.csv\",          \"data/interim/tableau/nc_city_ZHVI_2015present.csv\")\n",
    "tableau_proof(\"data/interim/nc_city_ZORI_2015present.csv\",          \"data/interim/tableau/nc_city_ZORI_2015present.csv\")\n",
    "tableau_proof(\"data/interim/nc_city_Inventory_2015present.csv\",     \"data/interim/tableau/nc_city_Inventory_2015present.csv\")\n",
    "tableau_proof(\"data/interim/nc_city_DaysToPending_2015present.csv\", \"data/interim/tableau/nc_city_DaysToPending_2015present.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec186af4-8f59-44c4-9073-a595fc5c42a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
